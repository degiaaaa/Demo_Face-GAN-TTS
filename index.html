<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Face-GAN-TTS – Audio Demo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    html,body{height:100%;margin:0;font-family:system-ui,Segoe UI,Roboto,Helvetica,Arial,sans-serif;line-height:1.6;background:#fff;color:#111}
    main{max-width:1000px;margin:0 auto;padding:2rem 1rem}
    h1{text-align:center;margin-bottom:0.5rem;font-size:clamp(1.8rem,4vw,2.4rem)}
    #metadata{font-size:0.95rem;text-align:center;margin-bottom:2rem;color:#444}
    #metadata p{margin:0.2rem 0}

    figure{margin:0 auto 1.75rem;display:flex;flex-direction:column;align-items:center;width:min(100%,640px)}
    .spectrogram{width:100%;aspect-ratio:953/768;background:url("./demo_files/compare_gt_gan_scratch_finetune_2x2_spk1263_00003.png") center/contain no-repeat;border:1px solid #ddd;border-radius:0.5rem}
    figcaption{font-size:0.9rem;margin-top:0.5rem;color:#444;text-align:center}

    #abstract{max-width:750px;margin:0 auto 2.5rem}
    #abstract h2{margin-top:0;font-size:1.4rem}
    #abstract p{margin:0 0 1rem 0;text-align:justify}
    #keywords{font-style:italic;margin-top:-0.5rem;font-size:0.9rem}

    #conditions{max-width:640px;margin:0 auto 2rem;font-size:0.95rem;color:#222}
    #conditions ul{padding-left:0;list-style-type:none}
    #conditions li{margin-bottom:0.75rem}

    .audio-block{margin:1.5rem 0;display:flex;flex-direction:column;align-items:center;gap:0.5rem}
    audio{width:100%;max-width:640px;border-radius:0.5rem;background:rgba(255,255,255,0.85);padding:0.5rem}

    section{margin-top:3rem}
    h2{font-size:clamp(1.3rem,3vw,1.8rem);margin-bottom:0.75rem}

    ul{padding-left:1.25rem}
    li{margin-bottom:0.5rem}
    a{color:#0658c2}

    main > section{
      /* gleiche Breite und Außenabstand wie #abstract … */
      max-width:750px;          /* verhindert übergroße Zeilenbreite */  /* MDN: max-width */   /* :contentReference[oaicite:0]{index=0} */
      margin:0 auto 2.5rem;     /* Top/Bottom = 0 / 2.5 rem, zentriert */
    }
    main > section > h2{
      margin-top:0;             /* identisch zu #abstract h2 */           /* MDN: margin‑top */   /* :contentReference[oaicite:1]{index=1} */
      font-size:1.4rem;         /* gleicht Schriftgröße an */
    }
  </style>
</head>
<body>
  <main>
    <h1>Face-GAN-TTS: An Adversarial-Diffusion Framework for Generating High-Quality Voices from Faces</h1>

    <!-- Author & affiliation -->
    <div id="metadata">
      <p>Deborah Guaia</p>
      <p>Cognitive Modeling Group, Wilhelm‑Schickard Institute for Computer Science, University of Tübingen</p>
      <p><time datetime="2025-07-28">July&nbsp;28, 2025</time></p>
      <p><em>Reviewers:</em> Prof.&nbsp;Dr.&nbsp;Martin Butz &nbsp;|&nbsp; Prof.&nbsp;Dr.&nbsp;Felix Wichmann</p>
      <p><em>Supervisor:</em> Dr.&nbsp;Christian Gumbsch</p>
    </div>

    <!-- Abstract -->
    <section id="abstract">
      <h2>Abstract</h2>
      <p>Face-conditioned text-to-speech (TTS) models open new perspectives for cognitive
science experiments, but so far suffer from audible noise and limited quality. While
the diffusion model FACE-TTS produces voice-specific prosody from a portrait and
text, it also creates noisy artifacts that complicate fine-grained perception studies.</p>
<p> This thesis focuses on overcoming these limitations by improving FACE-TTS
using an adversarial approach. To this end, Face-GAN-TTS is presented, a sys-
tem that optimizes the baseline model using a spectral discriminator. To assess
the quality of the generated voices, a listening study and voice analyses are con-
ducted. Training is conducted on 7,358 labeled speakers from the LRS2 dataset.
Evaluation is carried out on LRS2 and out-of-domain faces from the Chicago Face
Database (CFD).</p>
<p> Forty-five participants each listened to ten audio samples from Face-GAN-TTS
and FACE-TTS. The mean opinion score (MOS) for perceived quality increases sta-
tistically significantly for Face-GAN-TTS compared to baseline. The quality rises
from 2.88 to 3.19 on LRS2 and from 2.30 to 2.56 on CFD faces. At the same time,
perceived ”scratchiness“ decreases for Face-GAN-TTS statistically significantly by
about 0.7 MOS points for both domains. In general, Face-GAN-TTS reduces noise
and increases voice quality.</p>
<p> However, ablation studies suggest that the underlying cross-modal biometric
model uses background noise as identity cues. This means that speaker embed-
dings may not match the face and voice. Nevertheless, Face-GAN-TTS produces
clear, low-noise voices using only faces and text. This serves as a basis for low-
effort perception studies and multimodal research.</p>
    </section>
    <section>
    <h2>Ablation Studies</h2>
    <!-- Mel-spectrogram figure -->
    <figure>
      <div class="spectrogram" role="img" aria-label="Comparison of different training variants in the Mel spectrogram"></div>
      <figcaption>Figure&nbsp;1: Mel‑spectrogram comparison – (a) Ground Truth, (b) Face‑GAN‑TTS, (c) FACE‑TTS Scratch, (d) FACE‑TTS Finetuned, (e) FACE‑TTS trained on LRS3.</figcaption>
    </figure>

    <!-- Condition descriptions -->
    <div id="conditions">
      <ul>
        <li><strong>(a) Ground Truth</strong>: Reference recording of natural human speech.</li>
        <li><strong>(b) FACE‑GAN‑TTS</strong>: Generator pretrained on LRS3; full GAN setup finetuned on LRS2 with adversarial learning (learning rate <code>1e‑8</code>).</li>
        <li><strong>(c) FACE‑TTS Scratch</strong>: Baseline FACE‑TTS (Lee <em>et al.</em>, 2023) trained from scratch on LRS2, no adversarial learning (adversarial‑loss weight 0.7, learning rate <code>1e‑4</code>).</li>
        <li><strong>(d) FACE‑TTS Finetuned</strong>: Baseline FACE‑TTS pretrained on LRS3 and finetuned on LRS2 without GAN (learning rate <code>1e‑8</code>).</li>
        <li><strong>(e) FACE‑TTS LRS3‑only</strong>: Published baseline FACE‑TTS by Lee <em>et al.</em> (2023) trained exclusively on LRS3 (learning rate <code>1e‑4</code>, speaker weight γ = 0.01).</li>
      </ul>
    </div>
    </section>
    <section>
    <h2>Examples</h2>
    <!-- Audio samples -->
    <div class="audio-block">
      <strong>(b) Face‑GAN‑TTS</strong>
      <audio controls>
        <source src="demo_files/FACEGANTTS_spk1263_00003.wav" type="audio/wav" />
        Your browser does not support the audio element.
      </audio>
    </div>

    <div class="audio-block">
      <strong>(c) FACE‑TTS Scratch</strong>
      <audio controls>
        <source src="demo_files/FACETTS_scratch_spk1263_00003.wav" type="audio/wav" />
        Your browser does not support the audio element.
      </audio>
    </div>

    <div class="audio-block">
      <strong>(d) FACE‑TTS Finetuned</strong>
      <audio controls>
        <source src="demo_files/FACETTS_finetuned_spk1263_00003.wav" type="audio/wav" />
        Your browser does not support the audio element.
      </audio>
    </div>

    <div class="audio-block">
      <strong>(e) FACE‑TTS trained on LRS3</strong>
      <audio controls>
        <source src="demo_files/FACETTS_LRS3_ONLY_spk1263_00003.wav" type="audio/wav" />
        Your browser does not support the audio element.
      </audio>
    </div>
    </section>
    <!-- Ethical considerations -->
    <section id="ethics">
      <h2>Ethical Considerations</h2>
      <ul>
        <li><strong>Privacy &amp; Consent:</strong> All faces originate from publicly available research datasets that permit demonstration and research use.</li>
        <li><strong>Deepfake Risks:</strong> The technology could be misused for deception. Every generated voice is water‑marked and is <em>not</em> intended to impersonate real people.</li>
        <li><strong>Bias &amp; Fairness:</strong> We evaluate the model on the Chicago Face Database to expose potential biases with respect to gender, age, and ethnicity.</li>
        <li><strong>Responsible Release:</strong> Audio and image data are provided solely for scientific reproduction—no commercial use without explicit permission.</li>
      </ul>
    </section>

    <!-- References -->
    <section id="references">
      <h2>Referenced Projects &amp; Citations</h2>
      <ul>
        <li>Lee <em>et al.</em>, “<em>Lee, J., Chung, J. S., and Chung, S.-W. (2023). Imaginary voice: Face-styled diffusion model for text-to-speech. <a href="https://facetts.github.io">Project page</a></li>
        <li>Kong, J., Kim, J., and Bae, J. (2020a). Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. Advances in neural information processing systems, 33, 17022–17033.</li>
        <li>Ko, M., Kim, E., and Choi, Y.-H. (2024). Adversarial training of denoising diffusion model using dual discriminators for high-fidelity multi-speaker tts. IEEE Open Journal of Signal Processing, 5, 577–587.</li>
        <li>Ma, D. S., Correll, J., and Wittenbrink, B. (2015). The chicago face database: A free stimulus set of faces and norming data. Behavior research methods, 47, 1122–1135.</li>
        <li>Afouras, T., Chung, J. S., Senior, A. W., Vinyals, O., and Zisserman, A. (2018a). Deep audio-visual speech recognition. CoRR, abs/1809.02108.</li>
      </ul>
      <p>The full source code for this demo is available on <a href="https://github.com/degiaaaa/facetts-optimizer">GitHub</a>.</p>
    </section>

  </main>
</body>
</html>
